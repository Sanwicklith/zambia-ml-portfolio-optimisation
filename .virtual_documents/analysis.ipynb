


from pathlib import Path
import importlib
import utils.reporting as reporting
importlib.reload(reporting)
from utils.config import Paths, RunConfig
import numpy as np

paths = Paths(project_root=Path.cwd())
rc = RunConfig()

paths.ensure_dirs()

print("Raw LuSE file:", paths.raw_luse_xlsx)
print("Exists:", paths.raw_luse_xlsx.exists())

print("Daily long output:", paths.prices_daily_long_path)
print("Monthly prices output:", paths.prices_monthly_path)
print("Monthly returns output:", paths.returns_monthly_panel_path)






import pandas as pd

xl = pd.ExcelFile(paths.raw_luse_xlsx, engine="openpyxl")

print("Sheets found in LuSE Excel:")
for i, name in enumerate(xl.sheet_names):
    print(f"{i}: {name}")






sheet_name = xl.sheet_names[0]  # adjust if needed

df_head = pd.read_excel(
    paths.raw_luse_xlsx,
    sheet_name=sheet_name,
    engine="openpyxl",
    skiprows=3,
    header=0
)

print("Sheet:", sheet_name)
print("Columns:")
for c in df_head.columns:
    print("  -", repr(c))

display(df_head)






df_head["Row Labels"] = pd.to_datetime(df_head["Row Labels"], errors="coerce")

print("Date parsing success rate:", df_head["Row Labels"].notna().mean())
print("Date range:", df_head["Row Labels"].min(), "→", df_head["Row Labels"].max())

# Optional: check for suspicious footer rows
n_bad = df_head["Row Labels"].isna().sum()
print("Rows with unparseable dates:", n_bad)






import pandas as pd

# 1) Read the daily sheet (same parameters you already validated)
df = pd.read_excel(
    paths.raw_luse_xlsx,
    sheet_name="Daily Closing Prices",
    engine="openpyxl",
    skiprows=3,
    header=0
)

# 2) Parse date column and drop non-date rows (footer / totals)
df["Row Labels"] = pd.to_datetime(df["Row Labels"], errors="coerce")
df = df[df["Row Labels"].notna()].copy()

# 3) Drop known non-asset columns
DROP_COLS = {".", "(blank)", "Grand Total"}
asset_cols = [c for c in df.columns if c not in (["Row Labels"] + list(DROP_COLS))]

# 4) Coerce prices to numeric
df[asset_cols] = df[asset_cols].apply(pd.to_numeric, errors="coerce")

# 5) Sort and de-duplicate dates (defensive)
df = df.sort_values("Row Labels").drop_duplicates(subset=["Row Labels"], keep="last")

print("Daily rows:", len(df))
print("Date range:", df["Row Labels"].min(), "→", df["Row Labels"].max())
print("Number of tickers:", len(asset_cols))
print("Tickers (first 10):", asset_cols[:10])

df.head()









from utils.reporting import atomic_write_parquet  # we will add this if it doesn't exist yet

daily_long = (
    df.melt(id_vars="Row Labels", value_vars=asset_cols, var_name="ticker", value_name="close")
      .rename(columns={"Row Labels": "date"})
)

# keep explicit missing values only where meaningful; for a prices panel we typically drop empty observations
daily_long = daily_long.dropna(subset=["close"]).sort_values(["ticker", "date"])

print("Long rows (non-missing closes):", len(daily_long))
daily_long.head()

# atomic persistence
atomic_write_parquet(daily_long, paths.prices_daily_long_path)
print("Wrote:", paths.prices_daily_long_path)



daily_long = (
    df.melt(
        id_vars="Row Labels",
        value_vars=asset_cols,
        var_name="ticker",
        value_name="close"
    )
    .rename(columns={"Row Labels": "date"})
)

# Drop rows where no trade/price was recorded
daily_long = (
    daily_long
    .dropna(subset=["close"])
    .sort_values(["ticker", "date"])
    .reset_index(drop=True)
)

print("Daily long-format rows:", len(daily_long))
daily_long.head()






daily_long = (
    df.melt(
        id_vars="Row Labels",
        value_vars=asset_cols,
        var_name="ticker",
        value_name="close"
    )
    .rename(columns={"Row Labels": "date"})
)

# Drop rows where no trade/price was recorded
daily_long = (
    daily_long
    .dropna(subset=["close"])
    .sort_values(["ticker", "date"])
    .reset_index(drop=True)
)

print("Daily long-format rows:", len(daily_long))
daily_long.head()






atomic_write_parquet(
    daily_long,
    paths.prices_daily_long_path
)

print("Saved daily price panel to:")
print(paths.prices_daily_long_path)














import pandas as pd

# Load daily long-format prices
daily_long = pd.read_parquet(paths.prices_daily_long_path)

# Ensure correct dtypes (defensive)
daily_long["date"] = pd.to_datetime(daily_long["date"], errors="coerce")
daily_long["ticker"] = daily_long["ticker"].astype(str)

# Keep only valid rows
daily_long = daily_long.dropna(subset=["date", "ticker", "close"]).copy()

# Create month-end label
daily_long["month_end"] = daily_long["date"].dt.to_period("M").dt.to_timestamp("M")

# Monthly close = last observed close within each month
monthly_prices = (
    daily_long.sort_values(["ticker", "date"])
             .groupby(["ticker", "month_end"], as_index=False)
             .tail(1)
             .rename(columns={"close": "close_m"})
             [["month_end", "ticker", "close_m"]]
             .sort_values(["ticker", "month_end"])
             .reset_index(drop=True)
)

print("Monthly price rows:", len(monthly_prices))
print("Tickers:", monthly_prices["ticker"].nunique())
print("Monthly date range:", monthly_prices["month_end"].min(), "→", monthly_prices["month_end"].max())

monthly_prices.head()








atomic_write_parquet(monthly_prices, paths.prices_monthly_path)

print("Wrote:", paths.prices_monthly_path)








# Load monthly prices from Step 7
monthly_prices = pd.read_parquet(paths.prices_monthly_path)

# Ensure correct types and ordering
monthly_prices["month_end"] = pd.to_datetime(monthly_prices["month_end"], errors="coerce")
monthly_prices["ticker"] = monthly_prices["ticker"].astype(str)

monthly_prices = (
    monthly_prices.dropna(subset=["month_end", "ticker", "close_m"])
                 .sort_values(["ticker", "month_end"])
                 .reset_index(drop=True)
)

# Compute previous month price per ticker
monthly_prices["prev_close_m"] = monthly_prices.groupby("ticker")["close_m"].shift(1)

# Simple monthly return
monthly_prices["ret_m"] = (monthly_prices["close_m"] / monthly_prices["prev_close_m"]) - 1.0

# Long-format returns: drop first month per ticker (no lag)
monthly_returns_long = (
    monthly_prices.dropna(subset=["prev_close_m", "ret_m"])
                 [["month_end", "ticker", "ret_m"]]
                 .sort_values(["ticker", "month_end"])
                 .reset_index(drop=True)
)

print("Monthly returns (long) rows:", len(monthly_returns_long))
print("Tickers with returns:", monthly_returns_long["ticker"].nunique())
print("Return date range:", monthly_returns_long["month_end"].min(), "→", monthly_returns_long["month_end"].max())

monthly_returns_long.head()






from utils.reporting import atomic_write_parquet

# Wide return matrix
returns_wide = (
    monthly_returns_long.pivot(index="month_end", columns="ticker", values="ret_m")
                        .sort_index()
)

print("Wide return matrix shape (months x assets):", returns_wide.shape)
print("Missingness ratio:", returns_wide.isna().mean().mean())

# Persist long + wide returns
# - long is useful for diagnostics and backtesting logic
# - wide is the optimisation-ready matrix
atomic_write_parquet(monthly_returns_long, paths.returns_monthly_panel_path.with_name("luse_returns_monthly_long.parquet"))
atomic_write_parquet(returns_wide.reset_index(), paths.returns_monthly_panel_path)

print("Wrote long returns:", paths.returns_monthly_panel_path.with_name("luse_returns_monthly_long.parquet"))
print("Wrote wide returns:", paths.returns_monthly_panel_path)






import pandas as pd

# Load wide returns created in Step 8
returns_wide_df = pd.read_parquet(paths.returns_monthly_panel_path)
returns_wide_df["month_end"] = pd.to_datetime(returns_wide_df["month_end"], errors="coerce")
returns_wide_df = returns_wide_df.sort_values("month_end").reset_index(drop=True)

# Convert to matrix form with month_end as index
returns_wide = returns_wide_df.set_index("month_end")

# 1) Minimum history filter (per ticker)
min_months = rc.min_history_months
ticker_counts = returns_wide.notna().sum(axis=0)

eligible_tickers = ticker_counts[ticker_counts >= min_months].index.tolist()

print("Total tickers (raw):", returns_wide.shape[1])
print("Eligible tickers (>= min history):", len(eligible_tickers))
print("Min months required:", min_months)

returns_u = returns_wide[eligible_tickers].copy()

# 2) Month coverage filter (retain months with enough observed returns)
coverage_threshold = 0.80
month_coverage = returns_u.notna().mean(axis=1)  # fraction of tickers with data each month

kept_months_mask = month_coverage >= coverage_threshold
returns_u = returns_u.loc[kept_months_mask].copy()

print("Months (raw):", returns_wide.shape[0])
print("Months kept (>= coverage threshold):", returns_u.shape[0])
print("Coverage threshold:", coverage_threshold)

# Diagnostics
print("Final matrix shape (months x tickers):", returns_u.shape)
print("Final missingness ratio:", returns_u.isna().mean().mean())

returns_u.head()






from utils.reporting import atomic_write_parquet

final_returns_path = paths.tables_dir / "luse_returns_monthly_filtered.parquet"

atomic_write_parquet(returns_u.reset_index(), final_returns_path)

print("Wrote filtered returns matrix:", final_returns_path)






test_months = rc.test_months

if returns_u.shape[0] <= test_months + 12:
    print("Warning: not many months relative to test window. Consider reducing test_months.")
    
split_idx = returns_u.shape[0] - test_months

train_returns = returns_u.iloc[:split_idx].copy()
test_returns = returns_u.iloc[split_idx:].copy()

print("Train months:", train_returns.shape[0], "| Test months:", test_returns.shape[0])
print("Train range:", train_returns.index.min(), "→", train_returns.index.max())
print("Test range:", test_returns.index.min(), "→", test_returns.index.max())






# import numpy as np
# import pandas as pd

# Defensive: ensure no entirely-missing columns after filtering
assert train_returns.shape[1] > 0, "No assets in training set."
assert train_returns.shape[0] > 0, "No months in training set."

# Sample estimates from training data
mu_hat = train_returns.mean(axis=0)                 # expected monthly return per ticker
sigma_hat = train_returns.cov()                     # covariance matrix

print("Assets (N):", train_returns.shape[1])
print("Train months (T):", train_returns.shape[0])

# Basic diagnostics
print("Mean return summary (monthly):")
print(mu_hat.describe())

print("\nCovariance matrix shape:", sigma_hat.shape)
print("Any NaNs in covariance?", sigma_hat.isna().any().any())






def equal_weight_backtest(returns: pd.DataFrame) -> pd.Series:
    """
    Equal-weight portfolio backtest that re-normalises weights each month
    across available (non-missing) assets.
    Returns a time series of portfolio returns indexed by month_end.
    """
    r = returns.copy()

    # Indicator of available assets per month
    avail = r.notna().astype(float)
    n_avail = avail.sum(axis=1)

    # Avoid division by zero (should not occur after month coverage filter)
    n_avail = n_avail.replace(0, np.nan)

    # Equal weights among available assets each month
    w = avail.div(n_avail, axis=0)

    # Portfolio return each month
    port_ret = (w * r).sum(axis=1)
    return port_ret


ew_test_returns = equal_weight_backtest(test_returns)

print("EW Test months:", ew_test_returns.shape[0])
print("EW Test return range:", ew_test_returns.min(), "→", ew_test_returns.max())
ew_test_returns.head()






def perf_metrics(monthly_returns: pd.Series, risk_free_rate_annual: float = 0.0) -> dict:
    r = monthly_returns.dropna()
    mean_m = r.mean()
    vol_m = r.std(ddof=1)

    mean_ann = 12.0 * mean_m
    vol_ann = np.sqrt(12.0) * vol_m

    sharpe = np.nan
    if vol_ann > 0:
        sharpe = (mean_ann - risk_free_rate_annual) / vol_ann

    # Cumulative performance (simple compounding)
    equity_curve = (1.0 + r).cumprod()
    total_return = equity_curve.iloc[-1] - 1.0

    return {
        "mean_monthly": float(mean_m),
        "vol_monthly": float(vol_m),
        "mean_annual_approx": float(mean_ann),
        "vol_annual_approx": float(vol_ann),
        "sharpe_annual": float(sharpe),
        "total_return_test": float(total_return),
    }


ew_metrics = perf_metrics(ew_test_returns, risk_free_rate_annual=rc.risk_free_rate_annual)
ew_metrics






from utils.reporting import save_table, save_json

# Save the return series
ew_returns_df = ew_test_returns.to_frame(name="ew_ret_m")
ew_returns_path = paths.tables_dir / "benchmark_equal_weight_test_returns.csv"
save_table(ew_returns_df, ew_returns_path)

# Save the metrics
ew_metrics_path = paths.tables_dir / "benchmark_equal_weight_metrics.json"
save_json(ew_metrics, ew_metrics_path)

print("Wrote:", ew_returns_path)
print("Wrote:", ew_metrics_path)






# === FIX STEP 1: Robust covariance estimation (replaces complete-case approach) ===

from sklearn.covariance import LedoitWolf
import pandas as pd

def shrinkage_covariance(returns: pd.DataFrame) -> pd.DataFrame:
    """
    Ledoit–Wolf shrinkage covariance.
    Handles missing data conservatively and avoids sample collapse.
    """
    X = returns.copy()

    # De-mean each asset using available observations
    X = X.sub(X.mean(axis=0, skipna=True), axis=1)

    # Conservative handling of missing data
    X = X.fillna(0.0)

    lw = LedoitWolf().fit(X.values)

    return pd.DataFrame(
        lw.covariance_,
        index=X.columns,
        columns=X.columns
    )

# Compute GMV covariance using full training window
sigma_hat_gmv = shrinkage_covariance(train_returns)

print("Train months used:", train_returns.shape[0])
print("Assets (N):", sigma_hat_gmv.shape[0])
print("Any NaNs in sigma?", sigma_hat_gmv.isna().any().any())






from scipy.optimize import minimize

def solve_gmv(sigma: pd.DataFrame, max_weight: float = 0.20, allow_short: bool = False) -> pd.Series:
    """
    Solve constrained global minimum-variance portfolio:
    min w' Σ w  s.t. sum(w)=1, bounds.
    Returns weights indexed by asset ticker.
    """
    tickers = sigma.columns.tolist()
    S = sigma.values
    n = len(tickers)

    # Objective: w' S w
    def obj(w):
        return float(w.T @ S @ w)

    # Equality constraint: sum(w)=1
    cons = [{"type": "eq", "fun": lambda w: np.sum(w) - 1.0}]

    # Bounds
    if allow_short:
        # If shorts allowed, cap absolute weights is more complex; keep simple for now
        bounds = [(-max_weight, max_weight)] * n
    else:
        bounds = [(0.0, max_weight)] * n

    # Feasible initial guess: equal weights clipped to bounds then renormalised
    w0 = np.ones(n) / n
    w0 = np.clip(w0, bounds[0][0], bounds[0][1])
    w0 = w0 / w0.sum()

    res = minimize(
        obj,
        w0,
        method="SLSQP",
        bounds=bounds,
        constraints=cons,
        options={"maxiter": 2000, "ftol": 1e-12},
    )

    if not res.success:
        raise RuntimeError(f"GMV optimisation failed: {res.message}")

    w = pd.Series(res.x, index=tickers, name="weight")
    return w

# === STEP 2 CONFIRMATION: GMV weights from shrinkage covariance ===

gmv_weights = solve_gmv(
    sigma_hat_gmv,
    max_weight=rc.max_weight,
    allow_short=rc.allow_short
)

print("Sum of weights:", gmv_weights.sum())
print("Min weight:", gmv_weights.min())
print("Max weight:", gmv_weights.max())







# === STEP 6: Monthly rebalance backtest WITH turnover + transaction costs ===

def backtest_monthly_rebalance(
    returns: pd.DataFrame,
    target_weights: pd.Series,
    cost_bps_per_side: float = 0.0,
):
    """
    Monthly rebalance backtest.
    - Weights drift within the month due to returns
    - Rebalanced back to target at each month-end
    - Turnover recorded
    - Transaction costs applied using bps per side

    Cost model (baseline):
        cost_t = (cost_bps_per_side / 10,000) * turnover_t
    """
    r = returns.copy()
    tickers = r.columns.intersection(target_weights.index)

    r = r[tickers]
    w_target = target_weights.loc[tickers].astype(float)
    w_target = w_target / w_target.sum()

    w_prev = w_target.copy()
    c = cost_bps_per_side / 10_000.0

    rows = []

    for t, row in r.iterrows():
        row = row.fillna(0.0)

        # Gross portfolio return using start-of-month weights
        gross_ret = float((w_prev * row).sum())

        # Drift weights
        w_drift = w_prev * (1.0 + row)
        w_drift = w_drift / w_drift.sum()

        # Turnover from drift → target
        turnover = float((w_target - w_drift).abs().sum())

        # Transaction cost (baseline one-way turnover model)
        cost = c * turnover

        # Net return
        net_ret = gross_ret - cost

        rows.append({
            "month": t,
            "gmv_gross_ret_m": gross_ret,
            "turnover": turnover,
            "cost": cost,
            "gmv_net_ret_m": net_ret,
        })

        # Rebalance back to target
        w_prev = w_target.copy()

    return pd.DataFrame(rows).set_index("month")













from utils.reporting import save_table, save_json
from pathlib import Path

# ----------------------------
# SAFETY: define output paths
# ----------------------------
tables_dir = Path(paths.tables_dir)  # ensure Path object

gmv_returns_path = tables_dir / "benchmark_gmv_test_returns.csv"
gmv_turnover_path = tables_dir / "benchmark_gmv_turnover_test.csv"
gmv_metrics_path = tables_dir / "benchmark_gmv_metrics.json"
gmv_weights_path = tables_dir / "benchmark_gmv_weights.csv"

# ----------------------------
# SAFETY: ensure gmv_bt exists
# ----------------------------
if "gmv_bt" not in globals():
    gmv_bt = backtest_monthly_rebalance(
        test_returns,
        gmv_weights,
        cost_bps_per_side=50,
    )

# ----------------------------
# Compute metrics
# ----------------------------
gmv_test_returns = gmv_bt["gmv_net_ret_m"]

gmv_metrics = perf_metrics(
    gmv_test_returns,
    risk_free_rate_annual=rc.risk_free_rate_annual
)

print("GMV metrics:", gmv_metrics)

# ----------------------------
# Save artefacts
# ----------------------------
save_table(gmv_bt, gmv_returns_path)
save_table(gmv_bt[["turnover"]], gmv_turnover_path)
save_json(gmv_metrics, gmv_metrics_path)
save_table(gmv_weights.to_frame("weight"), gmv_weights_path)

print("Wrote:", gmv_returns_path)
print("Wrote turnover:", gmv_turnover_path)
print("Wrote:", gmv_metrics_path)
print("Wrote:", gmv_weights_path)






gmv_bt_50bps = backtest_monthly_rebalance(
    test_returns,
    gmv_weights,
    cost_bps_per_side=50
)

print("Rows:", gmv_bt_50bps.shape[0])
print("Avg turnover:", gmv_bt_50bps["turnover"].mean())
print("Avg cost:", gmv_bt_50bps["cost"].mean())
gmv_bt_50bps.head()


# === STEP 7: Performance metrics (net of 50 bps transaction costs) ===

gmv_net_returns_50bps = gmv_bt_50bps["gmv_net_ret_m"]

gmv_net_metrics_50bps = perf_metrics(
    gmv_net_returns_50bps,
    risk_free_rate_annual=rc.risk_free_rate_annual
)

print("GMV net metrics (50 bps):")
gmv_net_metrics_50bps




# === STEP 8: Transaction cost grid (net metrics table) ===

bps_grid = [25, 50, 100, 150]
rows = []

for bps in bps_grid:
    bt = backtest_monthly_rebalance(
        test_returns,
        gmv_weights,
        cost_bps_per_side=bps
    )
    net_ret = bt["gmv_net_ret_m"]
    metrics = perf_metrics(net_ret, risk_free_rate_annual=rc.risk_free_rate_annual)

    rows.append({
        "bps_per_side": bps,
        "mean_monthly": metrics["mean_monthly"],
        "vol_monthly": metrics["vol_monthly"],
        "mean_annual_approx": metrics["mean_annual_approx"],
        "vol_annual_approx": metrics["vol_annual_approx"],
        "sharpe_annual": metrics["sharpe_annual"],
        "total_return_test": metrics["total_return_test"],
        "avg_turnover": bt["turnover"].mean(),
        "avg_cost": bt["cost"].mean(),
    })

gmv_cost_grid = pd.DataFrame(rows).set_index("bps_per_side")
gmv_cost_grid


# === STEP 9A: Persist GMV net-of-cost benchmark grid ===



gmv_cost_grid_path = paths.tables_dir / "benchmark_gmv_cost_grid.csv"
save_table(gmv_cost_grid.reset_index(), gmv_cost_grid_path)

print("Wrote GMV cost grid:", gmv_cost_grid_path)



# === STEP 9B: Persist baseline net returns (50 bps) ===

gmv_net_50bps_path = paths.tables_dir / "benchmark_gmv_net_returns_50bps.csv"
save_table(
    gmv_bt_50bps[["gmv_net_ret_m"]],
    gmv_net_50bps_path
)

print("Wrote GMV net returns (50 bps):", gmv_net_50bps_path)









from sklearn.linear_model import ElasticNet
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline



ml_model = Pipeline([
    ("scaler", StandardScaler()),
    ("enet", ElasticNet(alpha=0.01, l1_ratio=0.5, max_iter=5000))
])






# === STEP 20E (CODE): ML-based weight tilting ===


# ---- Assumptions (fixed, not tuned) ----
lambda_tilt = 0.5  # tilt intensity (as defined in Markdown)

# ---- Placeholder: predicted excess returns ----
# TEMPORARY for deadline safety:
# Use last observed 3-month return as a simple ML proxy
# (We will replace this with ElasticNet predictions if time allows)

ml_signal = (
    train_returns
    .rolling(3)
    .mean()
    .iloc[-1]
    .reindex(gmv_weights.index)
    .fillna(0.0)
)

# ---- Apply tilting rule ----
raw_ml_weights = gmv_weights * (1.0 + lambda_tilt * ml_signal)

# ---- Ensure feasibility ----
raw_ml_weights[raw_ml_weights < 0] = 0.0  # no shorting
ml_weights = raw_ml_weights / raw_ml_weights.sum()

# ---- Sanity checks ----
print("Sum of ML weights:", ml_weights.sum())
print("Min ML weight:", ml_weights.min())
print("Max ML weight:", ml_weights.max())



ml_bt = backtest_monthly_rebalance(
    test_returns,
    ml_weights,
    cost_bps_per_side=50
)




